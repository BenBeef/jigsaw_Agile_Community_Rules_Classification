"""
ä½¿ç”¨ vLLM åŠ è½½ Llama-3.1-8B-Instruct æ¨¡å‹è¿›è¡Œé«˜æ•ˆæ‰¹é‡æ¨ç†
vLLM æä¾›æ›´é«˜çš„ååé‡å’ŒçœŸæ­£çš„å¹¶å‘å¤„ç†èƒ½åŠ›
"""

import time
from vllm import LLM, SamplingParams

# æ¨¡å‹è·¯å¾„
MODEL_PATH = "/root/autodl-tmp/warm-start__grpo__think__Llama-3.1-8B-Instruct"

print("æ­£åœ¨ä½¿ç”¨ vLLM åŠ è½½æ¨¡å‹...")
print(f"æ¨¡å‹è·¯å¾„: {MODEL_PATH}")

# åˆå§‹åŒ– vLLM
llm = LLM(
    model=MODEL_PATH,
    dtype="bfloat16",  # ä½¿ç”¨ bfloat16
    trust_remote_code=True,
    gpu_memory_utilization=0.9,  # ä½¿ç”¨90%çš„GPUæ˜¾å­˜
    max_model_len=8192,  # æœ€å¤§åºåˆ—é•¿åº¦
)

print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆï¼")

# è·å–tokenizerç”¨äºç»Ÿè®¡
tokenizer = llm.get_tokenizer()


def format_prompt(user_message, system_prompt="You are an excellent content moderation expert."):
    """
    æ ¼å¼åŒ–ä¸º Llama-3.1 çš„å¯¹è¯æ ¼å¼
    """
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ]
    
    # ä½¿ç”¨tokenizerçš„chat template
    formatted_prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    return formatted_prompt


def batch_chat(questions_list, system_prompt="You are an excellent content moderation expert.", 
               max_new_tokens=1024, temperature=0.1, print_stats=True):
    """
    ä½¿ç”¨ vLLM æ‰¹é‡å¤„ç†å¤šä¸ªé—®é¢˜ï¼ˆçœŸæ­£çš„å¹¶å‘ï¼‰
    
    å‚æ•°:
        questions_list: é—®é¢˜åˆ—è¡¨
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: ç”Ÿæˆæ¸©åº¦
        print_stats: æ˜¯å¦æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    
    è¿”å›:
        ç»“æœåˆ—è¡¨
    """
    print(f"ğŸš€ å‡†å¤‡æ‰¹é‡å¤„ç† {len(questions_list)} ä¸ªé—®é¢˜...\n")
    
    # æ ¼å¼åŒ–æ‰€æœ‰prompt
    prompts = [format_prompt(q, system_prompt) for q in questions_list]
    
    # è®¡ç®—è¾“å…¥tokenæ•°
    input_token_counts = []
    for prompt in prompts:
        tokens = tokenizer.encode(prompt)
        input_token_counts.append(len(tokens))
    
    # è®¾ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=temperature,
        top_p=0.9,
        max_tokens=max_new_tokens,
    )
    
    # æ‰¹é‡ç”Ÿæˆï¼ˆvLLMä¼šè‡ªåŠ¨ä¼˜åŒ–æ‰¹å¤„ç†ï¼‰
    print("âš¡ å¼€å§‹æ‰¹é‡ç”Ÿæˆ...")
    start_time = time.time()
    
    outputs = llm.generate(prompts, sampling_params)
    
    elapsed_time = time.time() - start_time
    
    # å¤„ç†ç»“æœ
    results = []
    total_input_tokens = 0
    total_output_tokens = 0
    
    for i, output in enumerate(outputs):
        generated_text = output.outputs[0].text
        output_tokens = len(output.outputs[0].token_ids)
        input_tokens = input_token_counts[i]
        
        total_input_tokens += input_tokens
        total_output_tokens += output_tokens
        
        results.append({
            'question': questions_list[i],
            'answer': generated_text,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
        })
        
        if print_stats:
            print(f"\nã€é—®é¢˜ {i+1}/{len(questions_list)}ã€‘")
            if len(questions_list[i]) > 100:
                print(f"å†…å®¹é¢„è§ˆ: {questions_list[i][:100]}...")
            else:
                print(f"å†…å®¹: {questions_list[i]}")
            print("-" * 60)
            print(f"ğŸ“Š Token | è¾“å…¥: {input_tokens} | è¾“å‡º: {output_tokens}")
            print(f"ã€å›ç­”ã€‘{generated_text}")
            print("=" * 60)
    
    # æ€»ä½“ç»Ÿè®¡
    total_tokens = total_input_tokens + total_output_tokens
    throughput = total_output_tokens / elapsed_time
    avg_time_per_question = elapsed_time / len(questions_list)
    
    print(f"\nâœ… æ‰€æœ‰é—®é¢˜å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š æ€»ç»Ÿè®¡:")
    print(f"   - æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
    print(f"   - å¹³å‡æ¯é¢˜: {avg_time_per_question:.2f}ç§’")
    print(f"   - è¾“å…¥Tokenæ€»æ•°: {total_input_tokens}")
    print(f"   - è¾“å‡ºTokenæ€»æ•°: {total_output_tokens}")
    print(f"   - Tokenæ€»æ•°: {total_tokens}")
    print(f"   - ååé‡: {throughput:.2f} tokens/s")
    print(f"   - å¹³å‡ç”Ÿæˆé€Ÿåº¦: {total_output_tokens/elapsed_time:.2f} tokens/s")
    
    return results


# æµ‹è¯•æ–‡æœ¬
text = '''
You are given a community rule, several example cases with their judgments, and a user query to evaluate.

Your task: Determine whether the user query violates the given rule.

Instructions:
- Each example case contains an <output> tag with the judgment: "yes" means it violates the rule, "no" means it does not.
- Use these example cases as references to guide your judgment of the query.
- Provide your answer in the same format: <output>yes</output> or <output>no</output>

Rule:
No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.

=======================================================================================================
cases

case1ï¼š
"
If you could tell your younger self something different about sex, what would that be?

i AM IN A CONTEST TO WIN FUNDING FOR MY SEX POSITIVE FILM: VOTE HERE:

http://sheknows.offerpop.com/campaign/813112/entry/v144417
"

<output> yes </output>


case2:
"
Watch Golden Globe Awards 2017 Live Online in HD Coverage without ADS (VIP STREAMS)
=

HD STREAM QUALITY >>> [WATCH LINK1](http://forum.submitexpress.com/viewtopic.php?f=9&t=215858)
=

HD BROADCASTING QUALITY >>> [WATCH LINK1](http://forum.submitexpress.com/viewtopic.php?f=9&t=215858)
=

Mobile Compatibility: YES
=

NO ADS | NO ADS | ADS
=
"

<output> no <ouput>

=======================================================================================================
query:
Banks don't want you to know this! Click here to know more!

Does This body violate the rule
'''


if __name__ == "__main__":
    print("\n" + "="*60)
    print("Llama-3.1-8B-Instruct vLLM æ‰¹é‡æ¨ç†ç³»ç»Ÿ")
    print("="*60 + "\n")
    
    # ç¤ºä¾‹é—®é¢˜
    questions = [
        "ä½ å¥½ï¼è¯·ç”¨ä¸­æ–‡ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
        "ä½ å¥½ï¼è¯·ç”¨ä¸­æ–‡ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
        text,
        text,
        text,
        text,
        text,
        text,
    ]

    questions = questions + questions
    
    # æ‰¹é‡å¤„ç†æ‰€æœ‰é—®é¢˜
    results = batch_chat(questions)

