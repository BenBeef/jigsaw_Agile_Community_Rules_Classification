"""
ä½¿ç”¨é¢„è®­ç»ƒçš„ Llama-3.1-8B-Instruct æ¨¡å‹è¿›è¡Œé—®ç­”
"""

import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer

# æ¨¡å‹è·¯å¾„
MODEL_PATH = "/root/autodl-tmp/warm-start__grpo__think__Llama-3.1-8B-Instruct"

print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
print(f"æ¨¡å‹è·¯å¾„: {MODEL_PATH}")

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16,  # ä½¿ç”¨ bfloat16 ä»¥èŠ‚çœæ˜¾å­˜
    device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ°å¯ç”¨çš„ GPU
    low_cpu_mem_usage=True
)

print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆï¼")
print(f"æ¨¡å‹è®¾å¤‡: {model.device}")
print(f"æ¨¡å‹å‚æ•°é‡: {model.num_parameters() / 1e9:.2f}B")


def chat(user_message, system_prompt="You are an excellent content moderation expert.", max_new_tokens=1024, temperature=0.1, print_tokens=True):
    """
    ä½¿ç”¨ Llama-3.1 æ¨¡å‹è¿›è¡Œå¯¹è¯
    
    å‚æ•°:
        user_message: ç”¨æˆ·çš„é—®é¢˜
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§
        print_tokens: æ˜¯å¦æ‰“å°tokenæ•°é‡ç»Ÿè®¡
    
    è¿”å›:
        æ¨¡å‹çš„å›ç­”
    """
    # æ„å»º Llama-3.1 æ ¼å¼çš„å¯¹è¯æ¶ˆæ¯
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ]
    
    # ä½¿ç”¨åˆ†è¯å™¨çš„ chat template è¿›è¡Œæ ¼å¼åŒ–
    input_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # ç¼–ç è¾“å…¥
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
    
    # è·å–è¾“å…¥tokenæ•°é‡
    input_token_count = inputs['input_ids'].shape[1]
    
    # ç”Ÿæˆå›ç­”
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # è®°å½•ç»“æŸæ—¶é—´
    elapsed_time = time.time() - start_time
    
    # è·å–è¾“å‡ºtokenæ•°é‡ï¼ˆåªè®¡ç®—ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
    output_token_count = outputs.shape[1] - input_token_count
    total_token_count = outputs.shape[1]
    
    # è®¡ç®—é€Ÿåº¦ï¼ˆtokens per secondï¼‰
    tokens_per_second = output_token_count / elapsed_time if elapsed_time > 0 else 0
    
    if print_tokens:
        print(f"ğŸ“Š Tokenç»Ÿè®¡ | è¾“å…¥: {input_token_count} | è¾“å‡º: {output_token_count} | æ€»è®¡: {total_token_count} | è€—æ—¶: {elapsed_time:.2f}s | é€Ÿåº¦: {tokens_per_second:.2f} tokens/s")
    
    # è§£ç è¾“å‡ºï¼ˆåªè¿”å›ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    
    return generated_text


text = '''
You are given a community rule, several example cases with their judgments, and a user query to evaluate.

Your task: Determine whether the user query violates the given rule.

Instructions:
- Each example case contains an <output> tag with the judgment: "yes" means it violates the rule, "no" means it does not.
- Use these example cases as references to guide your judgment of the query.
- Provide your answer in the same format: <output>yes</output> or <output>no</output>

Rule:
No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.

=======================================================================================================
cases

case1ï¼š
"
If you could tell your younger self something different about sex, what would that be?

i AM IN A CONTEST TO WIN FUNDING FOR MY SEX POSITIVE FILM: VOTE HERE:

http://sheknows.offerpop.com/campaign/813112/entry/v144417
"

<output> yes </output>


case2:
"
Watch Golden Globe Awards 2017 Live Online in HD Coverage without ADS (VIP STREAMS)
=

HD STREAM QUALITY >>> [WATCH LINK1](http://forum.submitexpress.com/viewtopic.php?f=9&t=215858)
=

HD BROADCASTING QUALITY >>> [WATCH LINK1](http://forum.submitexpress.com/viewtopic.php?f=9&t=215858)
=

Mobile Compatibility: YES
=

NO ADS | NO ADS | ADS
=
"

<output> no <ouput>

=======================================================================================================
query:
Banks don't want you to know this! Click here to know more!

Does This body violate the rule
'''

if __name__ == "__main__":
    print("\n" + "="*60)
    print("Llama-3.1-8B-Instruct ä¸­æ–‡é—®ç­”ç³»ç»Ÿ")
    print("="*60 + "\n")
    
    # ç¤ºä¾‹é—®é¢˜
    questions = [
        "ä½ å¥½ï¼è¯·ç”¨ä¸­æ–‡ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
        "ä½ å¥½ï¼è¯·ç”¨ä¸­æ–‡ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
        text,
        text,
        text,
        text,
        text,
        text,
    ]
    
    # ä¸²è¡Œå¤„ç†æ‰€æœ‰é—®é¢˜ï¼ˆGPUæ¨¡å‹ä¸æ”¯æŒçœŸæ­£çš„å¹¶å‘ï¼‰
    print(f"ğŸš€ å¼€å§‹å¤„ç† {len(questions)} ä¸ªé—®é¢˜...")
    total_start = time.time()
    
    for i, question in enumerate(questions, 1):
        print(f"\nã€é—®é¢˜ {i}/{len(questions)}ã€‘å¼€å§‹å¤„ç†...")
        if len(question) > 100:
            print(f"å†…å®¹é¢„è§ˆ: {question[:100]}...")
        else:
            print(f"å†…å®¹: {question}")
        print("-" * 60)
        answer = chat(question)
        print(f"ã€å›ç­”ã€‘{answer}")
        print("=" * 60)
    
    total_time = time.time() - total_start
    print(f"\nâœ… æ‰€æœ‰é—®é¢˜å¤„ç†å®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’ | å¹³å‡æ¯é¢˜: {total_time/len(questions):.2f}ç§’")
    
